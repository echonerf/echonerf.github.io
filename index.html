<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Can NeRFs See Without Cameras?</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin="anonymous" referrerpolicy="no-referrer" />
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>

  <!-- <style>
    body {
      font-family: 'Noto Sans', sans-serif;
      font-size: 18px;
      line-height: 1.6;
      background: #fff;
    }
    .container {
      padding-left: 15rem;
      padding-right: 15rem;
    }
    .footer {
      background-color: #f5f5f5;
      padding: 2rem 1.5rem;
      position: relative;
      bottom: 0;
      width: 100%;
    }
    p, li {
    text-align: justify;
  }
  footer p {
  text-align: center !important;
  } 
  .hero .container p {
  text-align: center !important;
}
  </style> -->

  <style>
  /* ---- Layout constraints (override Bulma) ---- */
  :root{
    --container-max: 1010px;   /* tweak to taste: 760â€“960px works well */
    --side-pad: 24px;         /* safe side padding on mobile/desktop */
  }

  html, body{
    font-family: 'Noto Sans', sans-serif;
    font-size: 18px;
    line-height: 1.6;
    background:#fff;
    color:#111;
    overflow-x: hidden; /* prevent accidental horizontal scroll */
  }

  /* Hard override all Bulma containers */
  .container,
  .section > .container,
  .hero .container{
    width: 100% !important;
    max-width: var(--container-max) !important;
    margin-left: auto !important;
    margin-right: auto !important;
    padding-left: var(--side-pad) !important;
    padding-right: var(--side-pad) !important;
  }

  /* Spacing rhythm */
  section.section{ padding-top: .8rem; padding-bottom: .8rem; }
  .hero { background:#fff; }
  .hero .hero-body{ padding-top: 2.2rem; padding-bottom: 2rem; }

  /* Headings (a bit tighter than Bulma defaults) */
  .hero .title.is-1{ font-family:'Google Sans',sans-serif; font-size: 2.1rem; line-height:1.2;  }
  .title.is-3{ font-size: 1.6rem; line-height:1.35; margin-bottom: 0.9rem; }
  .title.is-4{ font-size: 1.2rem; line-height:1.35; margin: 0.9rem 0 0.4rem; }

  /* Text alignment */
  p, li{ text-align: justify; }
  .has-text-centered p{ text-align: center; }

  /* Media defaults */
  img, video, canvas{ max-width: 100%; height: auto; display: block; margin-left:auto; margin-right:auto; }

  /* Links row (paper/arXiv buttons) */
  .links-row{
    margin-top: 0.9rem;
    display: flex;
    flex-wrap: wrap;
    gap: 0.6rem;
    justify-content: center;
  }
  /* smaller buttons */
  .button.is-dark.is-normal{
    font-size: 0.95rem !important;
    padding: 0.5rem 0.9rem !important;
    border-radius: 999px !important; /* rounded pills */
  }
  .button .icon{ margin-right: 0.4rem; }

  /* Author block */
  /* NeurIPS badge subtle color */
  .subtitle-badge{
    font-size:1.05rem;
    margin-top:.15rem;                
    margin-bottom:.9rem;          
    font-weight: 700;        
    color: #374151;          
    /* color: #3730a3;           */
    letter-spacing:.2px;
  }


  /* Authors */
  .author-list{
    display:flex; flex-wrap:wrap; justify-content:center;
    gap:.2rem .3rem;            /* very tight spacing between items */
    font-size:1.02rem; line-height:1.35;
  }
  .author{ white-space:nowrap; } /* keep name+sup together */
  .author a{ color:#1a73e8; text-decoration:none; }
  .author a:hover{ text-decoration:underline; }
  .author sup{ position:relative; top:-0.45em; font-size:.7em; margin-left:2px; }

  /* Render commas with tight spacing; no comma after last author */
  .author:not(:last-child)::after{
    content:",";
    margin:0 .2rem 0 .15rem;   /* tight, natural comma spacing */
    color:#333;
  }

  
  .affiliations{ margin-top:.25rem; color:#666; font-size:.94rem; }
  sup{ font-size:.7em; top:-0.6em; }

  /* Columns: keep inside the narrow container and avoid overflow gaps */
  .columns{ margin-left: 0 !important; margin-right: 0 !important; }
  .column{ padding-left: 0.5rem; padding-right: 0.5rem; }

  /* Footer */
  .footer{ background:#f5f5f5; padding:1.2rem 1rem; }

  /* Mobile tweaks */
  @media (max-width: 768px){
    html, body{ font-size: 16px; }
    .hero .title.is-1{ font-size: 1.8rem; }
    .title.is-3{ font-size: 1.35rem; }
    .title.is-4{ font-size: 1.1rem; }
    :root{ --side-pad: 18px; }
  }
</style>


</head>
<body>

<!-- <section class="hero">
  <div class="hero-body">
    <div class="container has-text-centered">
      <h1 class="title is-1">Can NeRFs See Without Cameras?</h1>
      <p style="font-size: 1.1rem; color: #555; font-weight: 500; margin-top: 0.5rem;">
        In submission to NeurIPS 2025
      </p>
      <p style="font-size: 1rem; margin-top: 1rem;">Few visualizations are shown below, especially GIF animations that are difficult to include in the paper.</p>
    </div>
  </div>
</section> -->
<section class="hero">
  <div class="hero-body">
    <div class="container has-text-centered">
      <h1 class="title is-1">Can NeRFs "See" Without Cameras?</h1>
      <div class="subtitle-badge">NeurIPS 2025</div>
        <div class="author-block">
          <div class="author-list">
            <span class="author"><a href="https://achaitanya.web.illinois.edu/">Chaitanya Amballa</a><sup>1</sup></span>
            <span class="author"><a href="https://basusattwik.github.io/">Sattwik Basu</a><sup>1*</sup></span>
            <span class="author"><a href="https://yulinlw2.web.illinois.edu/">Yu-Lin Wei</a><sup>1*</sup></span>
            <span class="author"><a href="#">Zhijian Yang</a><sup>2</sup></span>
            <span class="author"><a href="#">Mehmet Ergezer</a><sup>2</sup></span>
            <span class="author"><a href="https://croy.web.engr.illinois.edu/">Romit Roy Choudhury</a><sup>1,2</sup></span>
          </div>
          <div class="affiliations">
            <sup>1</sup> University of Illinois Urbana-Champaign; <sup>2</sup> Amazon
          </div>
          <div class="affiliations">
            <sup>*</sup> <span style="font-size: 0.85em;">Equal contribution</span>
          </div>
        </div>

      <!-- old stuff -->
      <!-- <div class="links-row">
        <a class="button is-dark is-medium" href="https://arxiv.org/pdf/2505.22441" target="_blank">
          <span class="icon">ðŸ“„</span><span>Paper</span>
        </a> -->
        <!-- Add more when ready: Code, Video, Data
        <a class="button is-light is-medium" href="#" target="_blank"><span class="icon">ðŸ’»</span><span>Code</span></a>
        <a class="button is-light is-medium" href="#" target="_blank"><span class="icon">ðŸŽ¬</span><span>Video</span></a>
        <a class="button is-light is-medium" href="#" target="_blank"><span class="icon">ðŸ“¦</span><span>Data</span></a>
        -->
      <!-- </div> -->

      <div class="links-row">
        <a class="button is-dark is-normal is-rounded"
          href="https://arxiv.org/pdf/2505.22441" target="_blank" rel="noopener">
          <span class="icon">ðŸ“„</span><span>Paper</span>
        </a>
        <a class="button is-dark is-normal is-rounded"
          href="https://arxiv.org/abs/2505.22441" target="_blank" rel="noopener">
          <span class="icon"><i class="ai ai-arxiv"></i></span>
          <span>arXiv</span>
        </a>
      </div>


      <!-- <p style="font-size: 1rem; margin-top: 1rem;">
        Few visualizations are shown below, especially GIF animations that are difficult to include in the paper.
      </p> -->
    </div>
  </div>
</section>

<section id="compare" class="section">
  <div class="container">
    <h3 class="title is-3">1. NeRFs can see with cameras, can we generalize them to RF signals?</h3>
    <!-- <h3 class="title is-3">2. EchoNeRF: Generalizing NeRFs to multipath signals</h3> -->
    <img src="./static/images/intro5.png" style="width: 100%; margin-top: 2em; margin-bottom: 2em;" alt="Left: vanilla NeRFâ€”cameras capture light from an object (e.g., a bulldozer). Right: EchoNeRFâ€”phones capture RF multipath echoes from a house." />
    <div>
      <p>
        In <strong> NeRFs</strong> (left), the sensors are <em>cameras</em> that record <em>light</em> (as RGB images) coming from an object of interest (bulldozer).
        From these multi-view RGB images, (optical) NeRFs learn to synthesize novel views of the object and reconstruct its shape.
        Similarly, in <strong>EchoNeRF</strong> (right), the sensors are <em>phones</em> that measure <em>RF signals</em> (as signal strength)
        present in an environment (the house), except that the measurements are now made inside the environment as opposed to outside.
        Given a few such RF measurements, EchoNeRF learns to infer the spatial layout of the house.
        <!-- While NeRFs use cameras to see objects, EchoNeRF uses RF signals to see environments. -->
        </p>
        <p style="margin-top: 1em; font-weight: bold;">TL;DR: NeRFs use cameras to see objects, EchoNeRF uses RF signals to see environments.</p>
    </div>
  </div>
</section>

<section class="section">
  <div class="container">
    <h3 class="title is-3">2. Challenges</h3>
    <img src="./static/images/overview_5.png" style="width: 100%; margin-bottom: 1em;" alt="Overview of Camera NeRFs vs EchoNeRF">
    <div>
      <p>
        A camera pixel (left)  primarily captures light arriving directly from the object in front of itâ€”along the line-of-sight (LoS) path (see <a href="#nerf">[1]</a>).
        In contrast, an RF sensor (right) such as a phone receives the LoS path and multiple <em>unknown</em> reflections, or "echoes," from walls and other objects in the surroundings.
        Previous works <a href="#nerf2">[2]</a>, <a href="#newrf">[3]</a> attempted to reuse NeRF-style reasoning for RF signals; although signal prediction was demonstrated, <span style="color: red;  font-weight: bold;">the core inverse problem of reconstructing geometry remains unsolved.</span>
        EchoNeRF redesigns NeRFs to learn from unknown multipath signals, enabling it to "see" the environment, thereby solving the inverse problem.
      </p>
    </div>
</section>


<section class="section">
  <div class="container">
    <h3 class="title is-3">3. Contribution</h3>
        <p>
          In EchoNeRF, every voxel is parameterized by its opacity \(\delta\) and a (discrete) orientation \(\omega\).
          Using these quantities, we design both the line of sight and first-order reflected signals analytically,
          which are then aggregated to form an approximation of the received signal \(\psi^*\).
        </p>
        <p style="text-align:center;">
          \[
          \tilde{\psi} = \psi_{\mathrm{LoS}} + \psi_{\mathrm{ref}_1}
          \]
        </p>

        <p style="text-align:center;">
          \[
          \psi_{\mathrm{LoS}} = K \;\frac{\displaystyle \prod_{\{i \mid v_i \in \mathrm{LoS}\}} \big(1-\delta_i\big)}{d^2}
          \]
        </p>

        <p style="text-align:center;">
          \[
          \psi_{\mathrm{ref}_1} = \displaystyle \sum_{\{j \mid v_j \in \mathcal{V}\}} \psi_{\mathrm{ref}}(v_j)
          \]
        </p>

        <p style="text-align:center;">
          \[
          \psi_{\mathrm{ref}}(v_j) = \delta_{j}\; f(\theta,\beta)\;
          \frac{\displaystyle
            \prod_{k \in \{ \mathrm{Rx}: v_j \}} \big(1-\delta_{k}\big)\;
            \prod_{l \in \{ v_j : \mathrm{Tx} \}} \big(1-\delta_{l}\big)
          }{\left(d_{\mathrm{Tx}:v_j} + d_{v_j:\mathrm{Rx}}\right)^2}
          \]
        </p>

        <p>
          We also introduce a two-stage training algorithm to stabilize the line-of-sight domination in the received signal.
        </p>
</section>




<section class="section">
  <div class="container">
    <h3 class="title is-3">4. Results</h3>
    <img src="./static/images/main_res2.png" style="width: 100%; margin-bottom: 1em;" alt="Overview of Camera NeRFs vs EchoNeRF">
    <div>
      <p>
        We evaluate EchoNeRF on the Zillow Indoor Dataset <a href="#zind">[4]</a>, which contains real-world floorplans.
        The above figure shows a qualitative comparison of ground truth floorplans against baselines. In the first row, <span style="color: red;  font-weight: bold;">red</span> 
        stars denote Tx locations and <span style="color: lightgray;  font-weight: bold;">light gray</span>  dots denote Rx measurement locations. The bottom two rows
        show floorplans learnt by our Stage 1 and Stage 2 models with sharper walls and boundaries.
      </p>
    </div>
</section>






<section class="section">
  <div class="container">
    <h2 class="title is-3">5. Downstream tasks can EchoNeRF solve</h2>

    <div class="content">
      <!-- <p>To demonstrate that EchoNeRF solves the true inverse problem, we perform the following tasks: -->
      <p>Because EchoNeRF solves the inverse problem, it allows for several downstream tasks:</p>
      <ul style="margin-left: 1.5rem;">
        <li>
          <strong>Signal Power (RSSI) Prediction:</strong> 
          We use the EchoNeRF's signal model to predict the RSSI at uniform grid locations across the predicted floorplan. We do this for the transmitters (Tx) used during training (<span style="color: red;  font-weight: bold;">red</span> star) as well as for new, unseen Tx locations (<span style="color: green;  font-weight: bold;">green</span> star).
          Observe that baselines overfit to the training Tx locations and fail to generalize to new Tx locations.
          EchoNeRF, on the other hand, generalizes well to unseen Tx locations.
        </li>
        <img src="./static/images/rssi_pred.png" style="width: 70%; margin-top: 1em; margin-bottom: 1em;" alt="Overview of Camera NeRFs vs EchoNeRF">

        <li>
          <strong>Channel Impulse Response (CIR) Prediction:</strong> 
          We randomly select a Tx-Rx pair inside the predicted floorplan and generate the CIRs using the NVIDIA's Sionna Simulator  <a href="#sionna">[5]</a>. 
        </li>
              <div class="columns" style="margin-top: 0.5em; margin-bottom: 1em;">
                <div class="column"><img src="./static/images/2_pred_channel_impulse_response_new.png" style="width: 100%;" alt="RIR PP 1"></div>
                <div class="column"><img src="./static/images/3_pred_channel_impulse_response_new.png" style="width: 100%;" alt="RIR PP 2"></div>
                <div class="column"><img src="./static/images/5_pred_channel_impulse_response_new.png" style="width: 100%;" alt="RIR PP 3"></div>
                <div class="column"><img src="./static/images/8_pred_channel_impulse_response_new.png" style="width: 100%;" alt="RIR PP 4"></div>
              </div>
        <li>
          <strong>Basic Ray tracing:</strong> 
          <!-- We simulate higher-order reflections (up to order 3) using the Sionna engine <a href="#sionna">[5]</a>. Ray tracing visuals below show Tx-Rx locations and echo paths for both ground truth and predicted floorplans. -->
          We also show the ray tracing results by generating higher order reflections (until order 3) using Sionna on the predicted floorplans.
          <!-- Tx-Rx pairs can be found by observing the ray intersections. -->
           Tx and Rx are shown in the image as <span style="color: red;  font-weight: bold;">red</span> and <span style="color: green;  font-weight: bold;">green</span> stars, respectively.
          <!-- Results are shown below. -->
        </li>
              <div class="columns is-gapless">
                <div class="column" style="padding: 0;">
                  <img src="./static/images/2_new_pred_scene.png"
                      style="width: 100%; padding-left: 30px; padding-right: 30px;"
                      alt="RIR PP 1">
                </div>
                <div class="column" style="padding: 0;">
                  <img src="./static/images/3_new_pred_scene.png"
                      style="width: 100%; padding-left: 30px; padding-right: 30px;"
                      alt="RIR PP 2">
                </div>
                <div class="column" style="padding: 0;">
                  <img src="./static/images/5_new_pred_scene.png"
                      style="width: 100%; padding-left: 30px; padding-right: 30px;"
                      alt="RIR PP 3">
                </div>
                <div class="column" style="padding: 0;">
                  <img src="./static/images/8_new_pred_scene.png"
                      style="width: 100%; padding-left: 30px; padding-right: 30px;"
                      alt="RIR PP 4">
                </div>
              </div>
      </ul>
    </div>


  </div>
</section>


<section class="section">
  <div class="container">
    <h3 class="title is-3">References</h3>
    <ol style="padding-left: 1.5rem; font-size: 1rem; line-height: 1.8;">
      <li id="nerf">
        B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, 
        "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis," 
        <em>ECCV</em>, 2020. 
        <a href="https://arxiv.org/abs/2003.08934" target="_blank">[arXiv]</a>
      </li>
      <li id="nerf2">
        X. Zhao, Z. An, Q. Pan, and L. Yang, "NeRF2: Neural Radio-Frequency Radiance Fields," 
        <em>Proceedings of the 29th Annual International Conference on Mobile Computing and Networking (MobiCom)</em>, 2023. 
        <a href="https://arxiv.org/abs/2305.06118" target="_blank">[arXiv]</a>
      </li>
      <li id="newrf">
        H. Lu, C. Vattheuer, B. Mirzasoleiman, and O. Abari, 
        "NeWRF: A Deep Learning Framework for Wireless Radiation Field Reconstruction and Channel Prediction," 
        in <em>Forty-first International Conference on Machine Learning (ICML)</em>, 2024.
        <a href="https://arxiv.org/abs/2403.03241" target="_blank">[arXiv]</a>
      </li>
      <li id="zind">
        S. Cruz, W. Hutchcroft, Y. Li, R. Martin-Brualla, D. B. Goldman, M. W. Turek, and S. Izadi, 
        "Zillow Indoor Dataset: Annotated Floor Plans with 360Â° Panoramas and 3D Room Layouts," 
        in <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2021. 
        <a href="https://docs.google.com/viewer?url=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent%2FCVPR2021%2Fpapers%2FCruz_Zillow_Indoor_Dataset_Annotated_Floor_Plans_With_360deg_Panoramas_and_CVPR_2021_paper.pdf" target="_blank">[PDF]</a>
      </li>
      <li id="sionna">
        J. Hoydis, S. Cammerer, F. Ait Aoudia, A. Vem, N. Binder, G. Marcus, and A. Keller, 
        "Sionna: An Open-Source Library for Next-Generation Physical Layer Research," 
        <em>arXiv preprint arXiv:2203.11854</em>, 2022. 
        <a href="https://arxiv.org/abs/2203.11854" target="_blank">[arXiv]</a>
      </li>
    </ol>
  </div>
</section>


<section class="section">
  <div class="container">
    <h3 class="title is-3">BibTeX</h3>
<pre style="overflow:auto; padding:1rem; background:#f7f7f7; border-radius:8px; border:1px solid #eee; font-size:0.9rem;">
    @inproceedings{echo-nerf-2025,
      title     = {Can NeRFs "See" Without Cameras?},
      author    = {Amballa, Chaitanya and Basu, Sattwik and Wei, Yu-Lin and Yang, Zhijian and Ergezer, Mehmet and Choudhury, Romit Roy},
      booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
      year      = {2025},
      url       = {https://arxiv.org/pdf/2505.22441},
      eprint    = {2505.22441},
      archivePrefix = {arXiv}
    }
</pre>
  </div>
</section>

<!-- <footer class="footer">
  <div class="content has-text-centered">
    <p>This site is anonymized for blind review. Last commit is made on 05/22.</p>
  </div>
</footer> -->

</body>
</html>
